{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_hans_dataset, load_mnli_mismatched_dataset, load_paws_qqp_dataset, and load_cola_ood_dataset\n",
    "\n",
    "#PAWS-QQP is not available in huggingface due to license of QQP. It must be reconstructed by downloading the original data and then running our scripts to produce the data and attach the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:This is a warning message\n",
      "ERROR:__main__:This is an error message\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Step 2: Configure the logging level\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "# Step 3: Optionally, you can customize the logging format\n",
    "logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.WARNING)\n",
    "\n",
    "# Step 4: Create a logger instance\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Now you can use the logger in your code\n",
    "logger.warning('This is a warning message')\n",
    "logger.error('This is an error message')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name=\"rte\"  #[\"rte\", \"mnli\", \"mnli-original\", \"qqp\", \"cola\"]\n",
    "pad_to_max_length=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load In-domain evaluation dataset (training data also included in the huggingface api call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function almost exact copy from paper's github repo\n",
    "def load_glue_datasets(task_name, use_auth_token,cache_dir=None):\n",
    "    # Get the datasets: specify a GLUE benchmark task (the dataset will be downloaded automatically from the datasets Hub).\n",
    "    #\n",
    "    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
    "    # download the dataset.\n",
    "\n",
    "    if task_name is not None:\n",
    "        if task_name == \"mnli\":\n",
    "            # convert to binary format (remove neutral class)\n",
    "            raw_datasets = load_dataset(\n",
    "                \"glue\", task_name, cache_dir=cache_dir)\n",
    "\n",
    "            raw_datasets = raw_datasets.filter(\n",
    "                lambda example: example[\"label\"] != 1)\n",
    "\n",
    "            # change labels of contradiction examples from 2 to 1\n",
    "            def change_label(example):\n",
    "                example[\"label\"] = 1 if example[\"label\"] == 2 else example[\"label\"]\n",
    "                return example\n",
    "            raw_datasets = raw_datasets.map(change_label)\n",
    "\n",
    "            # change features to reflect the new labels\n",
    "            features = raw_datasets[\"train\"].features.copy()\n",
    "            features[\"label\"] = ClassLabel(\n",
    "                num_classes=2, names=['entailment', 'contradiction'], id=None)\n",
    "            raw_datasets = raw_datasets.cast(\n",
    "                features)  # overwrite old features\n",
    "        \n",
    "        elif task_name == \"mnli-original\":\n",
    "            # convert to binary format (merge neutral and contradiction class)\n",
    "            raw_datasets = load_dataset(\n",
    "                path=\"glue\", name=\"mnli\", cache_dir=cache_dir)\n",
    "\n",
    "            # change labels of contradiction examples from 2 to 1\n",
    "            def change_label(example):\n",
    "                example[\"label\"] = 1 if example[\"label\"] == 2 else example[\"label\"]\n",
    "                return example\n",
    "            raw_datasets = raw_datasets.map(change_label)\n",
    "\n",
    "            # change features to reflect the new labels\n",
    "            features = raw_datasets[\"train\"].features.copy()\n",
    "            features[\"label\"] = ClassLabel(\n",
    "                num_classes=2, names=['entailment', 'contradiction'], id=None)\n",
    "            raw_datasets = raw_datasets.cast(\n",
    "                features)  # overwrite old features\n",
    "            \n",
    "        else:\n",
    "            # Downloading and loading a dataset from the hub.\n",
    "            raw_datasets = load_dataset(\n",
    "                \"glue\",\n",
    "                task_name,\n",
    "                cache_dir=cache_dir,\n",
    "                use_auth_token=True if use_auth_token else None,\n",
    "            )\n",
    "\n",
    "            if task_name == \"qqp\":\n",
    "                # we subsample qqp already here because its really big\n",
    "                # make sure we fix the seed here\n",
    "                np.random.seed(123)\n",
    "                for split in raw_datasets.keys():\n",
    "                    raw_datasets[split] = raw_datasets[split].select(np.random.choice(\n",
    "                        np.arange(len(raw_datasets[split])), size=1000, replace=False\n",
    "                    ))\n",
    "                    \n",
    "    # Determine number of labels\n",
    "    is_regression = task_name == \"stsb\"\n",
    "    if not is_regression:\n",
    "        label_list = raw_datasets[\"train\"].features[\"label\"].names\n",
    "        num_labels = len(label_list)\n",
    "    else:\n",
    "        num_labels = 1\n",
    "\n",
    "    return raw_datasets, label_list, num_labels, is_regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zijie-machine/Documents/DeepLearning/Project/env/lib/python3.11/site-packages/datasets/load.py:2554: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "raw_datasets, label_list, num_labels, is_regression = load_glue_datasets(task_name, use_auth_token=True, cache_dir=None)\n",
    "#raw datasets contains train, validation and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Out-of-domain evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function almost exact copy from paper's github repo\n",
    "def load_hans_dataset(cache_dir=None, heuristic=None, subcase=None, label=None):\n",
    "    # heuristic = {lexical_overlap, subsequence, constituent}\n",
    "    # subcase = see HANS_SUBCASES\n",
    "    # label = {0 (entailment), 1 (contradiction)}\n",
    "\n",
    "    subset = \"hans\"\n",
    "    dataset = load_dataset(\n",
    "        \"hans\", cache_dir=cache_dir, split=\"validation\")\n",
    "\n",
    "    # hans comes without indices, so we add them\n",
    "    indices = list(range(len(dataset)))\n",
    "    dataset = dataset.add_column(name=\"idx\", column=indices)\n",
    "\n",
    "    if heuristic is not None:  # filter dataset based on heuristic\n",
    "        dataset = dataset.filter(\n",
    "            lambda example: example[\"heuristic\"] == heuristic)\n",
    "        subset = f\"{subset}-{heuristic}\"\n",
    "\n",
    "    if subcase is not None:  # filter dataset based on subcase\n",
    "        dataset = dataset.filter(\n",
    "            lambda example: example[\"subcase\"] == subcase)\n",
    "        subset = f\"{subset}-{subcase}\"\n",
    "\n",
    "    if label is not None:  # filter dataset based on label\n",
    "        dataset = dataset.filter(\n",
    "            lambda example: example[\"label\"] == label)\n",
    "        subset = f\"{subset}-{'entailment' if label == 0 else 'contradiction'}\"\n",
    "\n",
    "    return dataset, subset\n",
    "\n",
    "def load_mnli_mismatched_dataset(label=None,cache_dir=None, merge=False):\n",
    "    subset = \"mnli_mm\"\n",
    "\n",
    "    dataset = load_dataset(\n",
    "        \"glue\", \"mnli\", split=f\"validation_mismatched\", cache_dir=cache_dir)\n",
    "\n",
    "    if not merge:\n",
    "        # remove neutral class\n",
    "        dataset = dataset.filter(\n",
    "            lambda example: example[\"label\"] != 1)\n",
    "\n",
    "    # change labels of contradiction examples from 2 to 1\n",
    "    def change_label(example):\n",
    "        example[\"label\"] = 1 if example[\"label\"] == 2 else example[\"label\"]\n",
    "        return example\n",
    "    dataset = dataset.map(change_label)\n",
    "\n",
    "    # change features to reflect the new labels\n",
    "    features = dataset.features.copy()\n",
    "    features[\"label\"] = ClassLabel(\n",
    "        num_classes=2, names=['entailment', 'contradiction'], id=None)\n",
    "    dataset = dataset.cast(\n",
    "        features)  # overwrite old features\n",
    "\n",
    "    if label is not None:  # filter dataset based on label\n",
    "        dataset = dataset.filter(\n",
    "            lambda example: example[\"label\"] == label)\n",
    "        subset = f\"{subset}-{'entailment' if label == 0 else 'contradiction'}\"\n",
    "\n",
    "    return dataset, subset\n",
    "\n",
    "def load_paws_qqp_dataset(path, label=None, cache_dir=None):\n",
    "    # TODO(mm): there's probably a better way of doing this\n",
    "    data_files = {\"validation\": path}\n",
    "    dataset = load_dataset(\"csv\", data_files=data_files,\n",
    "                           sep=\"\\t\", cache_dir=cache_dir)\n",
    "    dataset = dataset[\"validation\"]\n",
    "\n",
    "    subset = \"paws-qqp\"\n",
    "\n",
    "    def _clean_data(sample):\n",
    "        # the paws-qqp dataset was created as a stream of bytes. So every sentence starts with \"b and ends with \".\n",
    "        # we remove these\n",
    "        sample[\"sentence1\"] = sample[\"sentence1\"][2:-1]\n",
    "        sample[\"sentence2\"] = sample[\"sentence2\"][2:-1]\n",
    "        return sample\n",
    "\n",
    "    dataset = dataset.map(_clean_data, batched=False)\n",
    "    dataset = dataset.rename_column(\"id\", \"idx\")\n",
    "\n",
    "    if label is not None:  # filter dataset based on label\n",
    "        dataset = dataset.filter(\n",
    "            lambda example: example[\"label\"] == label)\n",
    "        subset = f\"{subset}-{'paraphrase' if label == 1 else 'not-paraphrase'}\"\n",
    "\n",
    "    return dataset, subset\n",
    "\n",
    "def load_cola_ood_dataset(path, label=None, cache_dir=None):\n",
    "    # TODO(mm): there's probably a better way of doing this\n",
    "    data_files = {\"validation\": path}\n",
    "    dataset = load_dataset(\"csv\", data_files=data_files, sep=\"\\t\", column_names=[\n",
    "                           'code', 'label', 'annotation', 'sentence'], cache_dir=cache_dir)\n",
    "    dataset = dataset[\"validation\"]\n",
    "\n",
    "    # cola-ood comes without indices, so we add them\n",
    "    indices = list(range(len(dataset)))\n",
    "    dataset = dataset.add_column(name=\"idx\", column=indices)\n",
    "\n",
    "    subset = \"cola-ood\"\n",
    "\n",
    "    if label is not None:  # filter dataset based on label\n",
    "        dataset = dataset.filter(\n",
    "            lambda example: example[\"label\"] == label)\n",
    "        subset = f\"{subset}-{'acceptable' if label == 1 else 'unacceptable'}\"\n",
    "\n",
    "    return dataset, subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ood_eval_datasets():\n",
    "        out_of_domain_eval_datasets = {}\n",
    "\n",
    "        for heuristic in [\"lexical_overlap\"]:\n",
    "                for label in [0, 1]:\n",
    "                        dataset, subset = load_hans_dataset(\n",
    "                                cache_dir=\"data\", heuristic=heuristic, subcase=None, label=label)\n",
    "                        print(f\"{subset}: {len(dataset)} examples\")\n",
    "                        out_of_domain_eval_datasets[subset] = dataset\n",
    "\n",
    "        for label in [0, 1]:\n",
    "                mnli_mm_subset, subset_name = load_mnli_mismatched_dataset(label=label)\n",
    "                out_of_domain_eval_datasets[subset_name] = mnli_mm_subset\n",
    "\n",
    "        for label in [0, 1]:\n",
    "                paws_qqp_subset, subset_name = load_paws_qqp_dataset(\n",
    "                path=\"/Users/zijie-machine/Documents/DeepLearning/Project/foundation/data/paws_qqp/dev_and_test.tsv\", label=label)\n",
    "                out_of_domain_eval_datasets[subset_name] = paws_qqp_subset\n",
    "\n",
    "        for label in [0, 1]:\n",
    "                cola_ood_subset, subset_name = load_cola_ood_dataset(\n",
    "                path=\"/Users/zijie-machine/Documents/DeepLearning/Project/foundation/data/cola_ood/dev.tsv\", label=label)\n",
    "                out_of_domain_eval_datasets[subset_name] = cola_ood_subset\n",
    "        return out_of_domain_eval_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hans-lexical_overlap-entailment: 5000 examples\n",
      "hans-lexical_overlap-contradiction: 5000 examples\n"
     ]
    }
   ],
   "source": [
    "out_of_domain_eval_datasets = load_ood_eval_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    # labels are: 0 (entailment), 1 (contradiction)\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-original\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mismatched\": (\"premise\", \"hypothesis\"),\n",
    "    \"hans\": (\"premise\", \"hypothesis\"),\n",
    "\n",
    "    # labels are: 0 (not_duplicate), 1 (duplicate)\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"paws-qqp\": (\"sentence1\", \"sentence2\"),\n",
    "\n",
    "    # labels are: 0 (not acceptable), 1 (acceptable)\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"cola-ood\": (\"sentence\", None),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_seq_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# We will pad later, dynamically at batch creation, to the max sequence length in each batch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmax_seq_length\u001b[49m \u001b[38;5;241m>\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mmodel_max_length:\n\u001b[1;32m      9\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe max_seq_length passed (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_seq_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is larger than the maximum length for the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer\u001b[38;5;241m.\u001b[39mmodel_max_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Using max_seq_length=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer\u001b[38;5;241m.\u001b[39mmodel_max_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     14\u001b[0m max_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(max_seq_length, tokenizer\u001b[38;5;241m.\u001b[39mmodel_max_length)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_seq_length' is not defined"
     ]
    }
   ],
   "source": [
    "sentence1_key, sentence2_key = task_to_keys[task_name]\n",
    "if pad_to_max_length:\n",
    "    padding = \"max_length\"\n",
    "else:\n",
    "    # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n",
    "    padding = False\n",
    "\n",
    "if max_seq_length > tokenizer.model_max_length:\n",
    "    logger.warning(\n",
    "        f\"The max_seq_length passed ({max_seq_length}) is larger than the maximum length for the\"\n",
    "        f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "    )\n",
    "\n",
    "max_seq_length = min(max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_eval_samples = None\n",
    "do_eval = True\n",
    "\n",
    "if do_eval:\n",
    "    eval_task_names = [task_name for task_name in out_of_domain_eval_datasets.keys()]\n",
    "    eval_datasets = [dataset for _, dataset in out_of_domain_eval_datasets.items()]\n",
    "\n",
    "    if max_eval_samples is not None:\n",
    "                # we fix the random seed that controls the sampling\n",
    "                # we need to uses a fixed seed here to make sure we evaluate on the same data\n",
    "                np.random.seed(123)\n",
    "\n",
    "                max_eval_samples = min(\n",
    "                    len(eval_dataset), max_eval_samples)\n",
    "                # randomly select a subset of the eval data\n",
    "                indices = np.random.choice(\n",
    "                    range(len(eval_dataset)), size=max_eval_samples, replace=False)\n",
    "                eval_dataset = eval_dataset.select(indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
