{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    set_seed,\n",
    "    AutoConfig,\n",
    "    PretrainedConfig,\n",
    "    EvalPrediction,\n",
    "    default_data_collator,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "# from transformers.utils import main_process_first\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch, wandb\n",
    "from datasets import load_dataset, ClassLabel\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "import numpy as np\n",
    "import random\n",
    "import datasets\n",
    "\n",
    "\n",
    "from load_data import load_glue_datasets, load_ood_eval_datasets\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_utils import task_to_keys, save_dataset\n",
    "\n",
    "config_name = None\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "task_name = \"rte\" #Options: rte, mnli, mnli-original, mnli-mismatched, hans, qqp, paws-qqp, cola, cola-ood \n",
    "\n",
    "padding = \"max_length\" #or \"max_length\" or False\n",
    "target_tokens = None #or \"ĠNo,ĠYes\"\n",
    "target_tokens_logits_only = False\n",
    "max_seq_length = 256\n",
    "pattern = \"{text1} ?\"\n",
    "data_seed = 42\n",
    "training_seed = 42\n",
    "do_train = True\n",
    "do_eval = True\n",
    "do_predict = False\n",
    "max_eval_samples = None\n",
    "max_predict_samples = None\n",
    "test_file = None\n",
    "overwrite_cache = False\n",
    "output_dir = None\n",
    "dataset_cache_dir = None\n",
    "pad_to_max_length = False\n",
    "fp16 = False\n",
    "main_process_first=True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zijie-machine/Documents/DeepLearning/Project/env/lib/python3.11/site-packages/datasets/load.py:2554: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hans-lexical_overlap-entailment: 5000 examples\n",
      "hans-lexical_overlap-contradiction: 5000 examples\n"
     ]
    }
   ],
   "source": [
    "raw_datasets, label_list, num_labels, is_regression = load_glue_datasets(task_name, use_auth_token=True, cache_dir=None)\n",
    "additional_evaluation_datasets = load_ood_eval_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = AutoConfig.from_pretrained(\n",
    "#     config_name if config_name else base_model,\n",
    "#     finetuning_task=task_name,\n",
    "#     num_labels=num_labels,\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# torch_dtype = torch.float16\n",
    "# attn_implementation = \"eager\"\n",
    "\n",
    "# # QLoRA config\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=False,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch_dtype,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "# # Load model\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     base_model,\n",
    "#     config=config,\n",
    "#     # quantization_config=bnb_config,\n",
    "#     device_map=\"auto\",\n",
    "#     attn_implementation=attn_implementation,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m sentence1_key, sentence2_key \u001b[38;5;241m=\u001b[39m task_to_keys[task_name]\n\u001b[1;32m      8\u001b[0m label_to_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlabel2id \u001b[38;5;241m!=\u001b[39m PretrainedConfig(\n\u001b[1;32m     11\u001b[0m         num_labels\u001b[38;5;241m=\u001b[39mnum_labels)\u001b[38;5;241m.\u001b[39mlabel2id\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m task_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_regression\n\u001b[1;32m     14\u001b[0m ):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Some have all caps in their config, some don't.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     label_name_to_id \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     17\u001b[0m         k\u001b[38;5;241m.\u001b[39mlower(): v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlabel2id\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28msorted\u001b[39m(label_name_to_id\u001b[38;5;241m.\u001b[39mkeys())) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28msorted\u001b[39m(label_list)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence1_key, sentence2_key = task_to_keys[task_name]\n",
    "\n",
    "label_to_id = None\n",
    "if (\n",
    "    model.config.label2id != PretrainedConfig(\n",
    "        num_labels=num_labels).label2id\n",
    "    and task_name is not None\n",
    "    and not is_regression\n",
    "):\n",
    "    # Some have all caps in their config, some don't.\n",
    "    label_name_to_id = {\n",
    "        k.lower(): v for k, v in model.config.label2id.items()}\n",
    "    if list(sorted(label_name_to_id.keys())) == list(sorted(label_list)):\n",
    "        label_to_id = {\n",
    "            i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}\n",
    "    else:\n",
    "        logger.warning(\n",
    "            \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n",
    "            f\"model labels: {list(sorted(label_name_to_id.keys()))}, dataset labels: {list(sorted(label_list))}.\"\n",
    "            \"\\nIgnoring the model labels as a result.\",\n",
    "        )\n",
    "\n",
    "if label_to_id is not None:\n",
    "    model.config.label2id = label_to_id\n",
    "    model.config.id2label = {\n",
    "        id: label for label, id in config.label2id.items()}\n",
    "elif task_name is not None and not is_regression:\n",
    "    model.config.label2id = {l: i for i, l in enumerate(label_list)}\n",
    "    model.config.id2label = {\n",
    "        id: label for label, id in config.label2id.items()}\n",
    "\n",
    "\n",
    "if target_tokens is not None and not target_tokens_logits_only:\n",
    "    # we need to convert the label ids to target ids\n",
    "    target_tokens = [t.strip() for t in target_tokens.split(\",\")]\n",
    "    target_tokens_ids = tokenizer.convert_tokens_to_ids(target_tokens)\n",
    "\n",
    "    model.config.label2id = {\n",
    "        l: target_tokens_ids[i] for i, l in enumerate(label_list)}\n",
    "    model.config.id2label = {\n",
    "        id: label for label, id in config.label2id.items()}\n",
    "\n",
    "# Compute max_seq_length\n",
    "if max_seq_length > tokenizer.model_max_length:\n",
    "    logger.warning(\n",
    "        f\"The max_seq_length passed ({max_seq_length}) is larger than the maximum length for the\"\n",
    "        f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "    )\n",
    "\n",
    "max_seq_length = min(max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "\n",
    "    # Apply a pattern to the inputs\n",
    "    pattern_examples = [\n",
    "        pattern.format(\n",
    "            text1=examples[sentence1_key][idx],\n",
    "            text2=examples[sentence2_key][idx] if sentence2_key is not None else None)\n",
    "        for idx in range(len(examples[sentence1_key]))\n",
    "    ]\n",
    "    args = (pattern_examples,)\n",
    "    result = tokenizer(*args, padding=padding,\n",
    "                        max_length=max_seq_length, truncation=True)\n",
    "\n",
    "    # Get mask for soft prompt tokens\n",
    "    # TODO(mm): For GPT-J and GPT-NeoX we have a different tokenizer. Adjust accordingly\n",
    "    # Get tokens\n",
    "    result[\"input_tokens\"] = [tokenizer.convert_ids_to_tokens(\n",
    "        ids) for ids in result[\"input_ids\"]]\n",
    "\n",
    "    # Decode input\n",
    "    result[\"input_text\"] = [tokenizer.decode(\n",
    "        ids) for ids in result[\"input_ids\"]]\n",
    "\n",
    "    # Replace labels by target tokens indices when using lm_head\n",
    "    # - special case: when using target logits only, we keep class indices instead of token indices\n",
    "    if target_tokens is not None and not target_tokens_logits_only:\n",
    "        result[\"label\"] = [target_tokens_ids[l] for l in examples[\"label\"]]\n",
    "    else:\n",
    "        result[\"label\"] = examples[\"label\"]\n",
    "\n",
    "    result[\"label_text\"] = [model.config.id2label[l] if l != -1 else \"unlabeled\"\n",
    "                            for l in result[\"label\"]]\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_tokens is not None and not target_tokens_logits_only:\n",
    "    for split in raw_datasets:\n",
    "        # raw_datasets[split].features[\"label\"].num_classes = len(tokenizer)\n",
    "        # raw_datasets[split].features[\"label\"].names = [\n",
    "        #     f\"{idx}\" for idx in np.arange(len(tokenizer))]\n",
    "\n",
    "        new_features = raw_datasets[split].features.copy()\n",
    "        names = [f\"{idx}\" for idx in np.arange(len(tokenizer))]\n",
    "        new_features[\"label\"] = ClassLabel(\n",
    "            names=names, num_classes=len(tokenizer))\n",
    "        raw_datasets[split] = raw_datasets[split].cast(new_features)\n",
    "\n",
    "    for name, dataset in additional_evaluation_datasets.items():\n",
    "        # dataset.features[\"label\"].num_classes = len(tokenizer)\n",
    "        # dataset.features[\"label\"].names = [\n",
    "        #     f\"{idx}\" for idx in np.arange(len(tokenizer))]\n",
    "\n",
    "        new_features = dataset.features.copy()\n",
    "        names = [f\"{idx}\" for idx in np.arange(len(tokenizer))]\n",
    "        new_features[\"label\"] = ClassLabel(\n",
    "            names=names, num_classes=len(tokenizer))\n",
    "        additional_evaluation_datasets[name] = dataset.cast(new_features)\n",
    "\n",
    "# before running the pre-processing, subsample datsets if specified\n",
    "\n",
    "# subsample datasets (if specified)\n",
    "\n",
    "# we fix the random seed that controls the sampling of the training data\n",
    "np.random.seed(data_seed)\n",
    "\n",
    "if do_train:\n",
    "    if \"train\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_train requires a train dataset\")\n",
    "    train_dataset = raw_datasets[\"train\"]\n",
    "    if max_train_samples is not None:\n",
    "        # randomly select a subset of the training data\n",
    "        max_train_samples = min(\n",
    "            len(train_dataset), max_train_samples)\n",
    "        indices = np.random.choice(\n",
    "            range(len(train_dataset)), size=max_train_samples, replace=False)\n",
    "        train_dataset = train_dataset.select(indices)\n",
    "\n",
    "if do_eval:\n",
    "    # we fix the random seed that controls the sampling of the validation data\n",
    "    np.random.seed(123)  # we only use this for debugging\n",
    "\n",
    "    if \"validation\" not in raw_datasets and \"validation_matched\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "    eval_dataset = raw_datasets[\"validation_matched\" if task_name in\n",
    "                                [\"mnli\", \"mnli-original\"] else \"validation\"]\n",
    "\n",
    "    # (optional) subsample eval datasets\n",
    "    if max_eval_samples is not None:\n",
    "        max_eval_samples = min(\n",
    "            len(eval_dataset), max_eval_samples)\n",
    "        # randomly select a subset of the eval data\n",
    "        indices = np.random.choice(\n",
    "            range(len(eval_dataset)), size=max_eval_samples, replace=False)\n",
    "        eval_dataset = eval_dataset.select(indices)\n",
    "\n",
    "    for name, dataset in additional_evaluation_datasets.items():\n",
    "        if max_eval_samples is not None:\n",
    "            max_eval_samples = min(\n",
    "                len(dataset), max_eval_samples)\n",
    "            # randomly select a subset of the eval data\n",
    "            indices = np.random.choice(\n",
    "                range(len(dataset)), size=max_eval_samples, replace=False)\n",
    "            dataset = dataset.select(indices)\n",
    "            additional_evaluation_datasets[name] = dataset\n",
    "\n",
    "if do_predict or task_name is not None or test_file is not None:\n",
    "    # we fix the random seed that controls the sampling of the validation data\n",
    "    np.random.seed(123)  # we only use this for debugging\n",
    "\n",
    "    if \"test\" not in raw_datasets and \"test_matched\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_predict requires a test dataset\")\n",
    "    predict_dataset = raw_datasets[\"test_matched\" if task_name in\n",
    "                                    [\"mnli\", \"mnli-original\"] else \"test\"]\n",
    "    if max_predict_samples is not None:\n",
    "        max_predict_samples = min(\n",
    "            len(predict_dataset), max_predict_samples)\n",
    "        predict_dataset = predict_dataset.select(\n",
    "            range(max_predict_samples))\n",
    "\n",
    "# set all random seeds again (not sure if this is really needed)\n",
    "set_seed(training_seed)\n",
    "\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "# tokenize and encode datasets\n",
    "\n",
    "with accelerator.main_process_first(desc=\"dataset map pre-processing\"):\n",
    "    if do_train:\n",
    "        train_dataset = train_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            load_from_cache_file=not overwrite_cache,\n",
    "            desc=\"Running tokenizer on training dataset\",\n",
    "        )\n",
    "\n",
    "    if do_eval:\n",
    "        eval_dataset = eval_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            load_from_cache_file=not overwrite_cache,\n",
    "            desc=\"Running tokenizer on validation dataset\",\n",
    "        )\n",
    "\n",
    "    if do_predict:\n",
    "        predict_dataset = predict_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            load_from_cache_file=not overwrite_cache,\n",
    "            desc=\"Running tokenizer on test dataset\",\n",
    "        )\n",
    "\n",
    "    for name, dataset in additional_evaluation_datasets.items():\n",
    "        if \"hans\" in name:\n",
    "            sentence1_key, sentence2_key = task_to_keys[\"hans\"]\n",
    "        elif \"mnli\" in name:\n",
    "            sentence1_key, sentence2_key = task_to_keys[\"mnli\"]\n",
    "        elif \"paws-qqp\" in name:\n",
    "            sentence1_key, sentence2_key = task_to_keys[\"paws-qqp\"]\n",
    "        elif \"cola-ood\" in name:\n",
    "            sentence1_key, sentence2_key = task_to_keys[\"cola-ood\"]\n",
    "\n",
    "        dataset = dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            load_from_cache_file=not overwrite_cache,\n",
    "            desc=f\"Running tokenizer on {name} validation dataset\",\n",
    "        )\n",
    "        additional_evaluation_datasets[name] = dataset\n",
    "\n",
    "# Log a few random samples from the training set:\n",
    "if do_train:\n",
    "    for index in random.sample(range(len(train_dataset)), 1):\n",
    "        print(\n",
    "            f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "# Log training and evaluation examples to training_args.output_dir for reproducibility\n",
    "if do_train:\n",
    "    save_dataset(train_dataset, path=os.path.join(\n",
    "        output_dir, f\"{task_name}-train.csv\"))\n",
    "if do_eval:\n",
    "    save_dataset(eval_dataset, path=os.path.join(\n",
    "        output_dir, f\"{task_name}-eval.csv\"))\n",
    "    for name, dataset in additional_evaluation_datasets.items():\n",
    "        save_dataset(dataset, path=os.path.join(\n",
    "            output_dir, f\"{name}-eval.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metric function\n",
    "if task_name is not None:\n",
    "    # use default metrics\n",
    "    metric_script = f\"{os.environ['PROJECT_DIR']}/metrics/glue.py\"\n",
    "    if task_name == \"mnli-original\":\n",
    "        metric = datasets.load_metric(path=metric_script, config_name=\"mnli\",\n",
    "                                        cache_dir=dataset_cache_dir, keep_in_memory=False)\n",
    "    else:\n",
    "        metric = datasets.load_metric(path=metric_script, config_name=task_name,\n",
    "                                        cache_dir=dataset_cache_dir, keep_in_memory=False)\n",
    "else:\n",
    "    metric = datasets.load_metric(\n",
    "        \"accuracy\", cache_dir=dataset_cache_dir, keep_in_memory=False)        \n",
    "\n",
    "# You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n",
    "# predictions and label_ids field) and has to return a dictionary string to float.\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(\n",
    "        p.predictions, tuple) else p.predictions\n",
    "    preds = np.squeeze(\n",
    "        preds) if is_regression else np.argmax(preds, axis=1)\n",
    "\n",
    "    if task_name is not None:\n",
    "        result = metric.compute(predictions=preds, references=p.label_ids)\n",
    "\n",
    "        # When using the lm_head, compute fraction of predictions that are not one of the target tokens\n",
    "        if target_tokens is not None and not target_tokens_logits_only:\n",
    "            unique_preds, counts_preds = np.unique(\n",
    "                preds, return_counts=True)\n",
    "            unique_preds_counts_dict = dict(\n",
    "                zip(unique_preds, counts_preds))\n",
    "\n",
    "            num_of_target_token_predictions = 0\n",
    "            for idx in target_tokens_ids:\n",
    "                num_of_target_token_predictions += unique_preds_counts_dict.get(\n",
    "                    idx, 0)\n",
    "            num_other_tokens = len(\n",
    "                preds) - num_of_target_token_predictions\n",
    "            result[\"frac_non_target_tokens\"] = num_other_tokens / \\\n",
    "                len(preds)\n",
    "\n",
    "        # # Combine eval metrics\n",
    "        # if len(result) > 1:\n",
    "        #     result[\"combined_score\"] = np.mean(\n",
    "        #         list(result.values())).item()\n",
    "\n",
    "        return result\n",
    "\n",
    "    elif is_regression:\n",
    "        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n",
    "    else:\n",
    "        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n",
    "\n",
    "# Data collator will default to DataCollatorWithPadding when the tokenizer is passed to Trainer, so we change it if\n",
    "# we already did the padding.\n",
    "if pad_to_max_length:\n",
    "    data_collator = default_data_collator\n",
    "elif fp16:\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer, pad_to_multiple_of=8)\n",
    "else:\n",
    "    data_collator = None\n",
    "\n",
    "# Initialize our Trainer\n",
    "if do_eval:\n",
    "    if len(additional_evaluation_datasets) > 0:\n",
    "        # add the training task eval dataset\n",
    "        additional_evaluation_datasets[task_name] = eval_dataset\n",
    "        eval_datasets = additional_evaluation_datasets\n",
    "    else:\n",
    "        eval_datasets = eval_dataset\n",
    "else:\n",
    "    eval_datasets = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = FtTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_datasets,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    data_args=data_args,\n",
    "    wandb_args=wandb_args,\n",
    "    ft_args=ft_args,\n",
    "    callbacks=None\n",
    ")\n",
    "\n",
    "# Training\n",
    "if do_train:\n",
    "    checkpoint = None\n",
    "    if resume_from_checkpoint is not None:\n",
    "        checkpoint = resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint, ignore_keys_for_eval=[\"past_key_values\"])\n",
    "    metrics = train_result.metrics\n",
    "    max_train_samples = (\n",
    "        max_train_samples if max_train_samples is not None else len(\n",
    "            train_dataset)\n",
    "    )\n",
    "    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "\n",
    "kwargs = {\"finetuned_from\": model_name_or_path,\n",
    "            \"tasks\": \"text-classification\"}\n",
    "if task_name is not None:\n",
    "    kwargs[\"language\"] = \"en\"\n",
    "    kwargs[\"dataset_args\"] = task_name\n",
    "\n",
    "if push_to_hub:\n",
    "    trainer.push_to_hub(**kwargs)\n",
    "else:\n",
    "    trainer.create_model_card(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "dataset_name = \"ruslanmv/ai-medical-chatbot\"\n",
    "new_model = \"llama-3-8b-chat-doctor\"\n",
    "\n",
    "torch_dtype = torch.float16\n",
    "attn_implementation = \"eager\"\n",
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    config=config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import CustomMetric\n",
    "\n",
    "metric_script = \"/Users/zijie-machine/Documents/DeepLearning/Project/llmft/metrics/glue.py\" \n",
    "\n",
    "# Get the metric function\n",
    "cm = CustomMetric(task_name=task_name, metric_script=metric_script)\n",
    "\n",
    "cm.compute_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, setup_chat_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=512,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
