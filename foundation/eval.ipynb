{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhsoh/miniconda3/envs/cs7643-llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/zhsoh/miniconda3/envs/cs7643-llm/lib/python3.12/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import datasets\n",
    "from datasets import ClassLabel, Value\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import evaluate\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "from options import DataTrainingArguments, ModelArguments, InContextLearningArguments, WandbArguments, FtArguments\n",
    "from utils import create_dir, get_timestamp\n",
    "from task_utils import task_to_keys, load_glue_datasets, load_hans_dataset, load_mnli_mismatched_dataset, load_paws_qqp_dataset, load_cola_ood_dataset\n",
    "from custom_trainer.ft_trainer import FtTrainer\n",
    "from eval_utils import create_few_shot_context, add_context_to_dataset, _select_subset_by_idx\n",
    "from llama_wrapper import LlamaWithLMClassifier\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Log in to Hugging Face\n",
    "hf_token = \"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\"\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_model(model_args: ModelArguments):\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "\n",
    "    # Add vanilla fine-tuning specific args to the model config\n",
    "    config.classifier_type = None\n",
    "\n",
    "    # Add pattern-verbalizer fine-tuning specific args to the model config\n",
    "    config.untie_embeddings = False\n",
    "\n",
    "    # Add adapter specific args to the model config\n",
    "    config.use_adapters = False\n",
    "    config.adapter_type = None\n",
    "    config.adapter_dim = None\n",
    "\n",
    "    # Add soft prompt tuning specific args to the model config\n",
    "    config.use_soft_prompt = False\n",
    "    config.num_soft_prompt_tokens = None\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_fast=model_args.use_fast_tokenizer,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    model = LlamaWithLMClassifier(config).from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        from_tf=False,\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "        ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_config,\n",
    "        attn_implementation=\"eager\",\n",
    "    )\n",
    "\n",
    "    peft_config = LoraConfig(task_type=TaskType.SEQ_CLS, \n",
    "                            inference_mode=False, \n",
    "                            lora_alpha=16,\n",
    "                            lora_dropout=0.05,\n",
    "                            r=16,\n",
    "                            bias=\"none\",\n",
    "                            target_modules=[\"q_proj\", \"v_proj\"])\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # We need to add a padding token for llama\n",
    "    tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "    config.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "    tokenizer.pad_token_id = config.pad_token_id\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # Update Model Embeddings:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    return config, tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m config, tokenizer, model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 39\u001b[0m, in \u001b[0;36m_load_model\u001b[0;34m(model_args)\u001b[0m\n\u001b[1;32m     24\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     25\u001b[0m     model_args\u001b[38;5;241m.\u001b[39mtokenizer_name \u001b[38;5;28;01mif\u001b[39;00m model_args\u001b[38;5;241m.\u001b[39mtokenizer_name \u001b[38;5;28;01melse\u001b[39;00m model_args\u001b[38;5;241m.\u001b[39mmodel_name_or_path,\n\u001b[1;32m     26\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mmodel_args\u001b[38;5;241m.\u001b[39mcache_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     use_auth_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m model_args\u001b[38;5;241m.\u001b[39muse_auth_token \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     33\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     34\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     35\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[1;32m     36\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     37\u001b[0m )\n\u001b[0;32m---> 39\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaWithLMClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     40\u001b[0m     model_args\u001b[38;5;241m.\u001b[39mmodel_name_or_path,\n\u001b[1;32m     41\u001b[0m     from_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     42\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     43\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mmodel_args\u001b[38;5;241m.\u001b[39mcache_dir,\n\u001b[1;32m     44\u001b[0m     revision\u001b[38;5;241m=\u001b[39mmodel_args\u001b[38;5;241m.\u001b[39mmodel_revision,\n\u001b[1;32m     45\u001b[0m     use_auth_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m model_args\u001b[38;5;241m.\u001b[39muse_auth_token \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     46\u001b[0m     ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mmodel_args\u001b[38;5;241m.\u001b[39mignore_mismatched_sizes,\n\u001b[1;32m     47\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m     48\u001b[0m     quantization_config\u001b[38;5;241m=\u001b[39mbnb_config,\n\u001b[1;32m     49\u001b[0m     attn_implementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     52\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(task_type\u001b[38;5;241m=\u001b[39mTaskType\u001b[38;5;241m.\u001b[39mSEQ_CLS, \n\u001b[1;32m     53\u001b[0m                         inference_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m     54\u001b[0m                         lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m                         bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     58\u001b[0m                         target_modules\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     59\u001b[0m model \u001b[38;5;241m=\u001b[39m get_peft_model(model, peft_config)\n",
      "File \u001b[0;32m/mnt/c/Users/Justus Soh/Desktop/CS7643/llm/foundation/llama_wrapper.py:22\u001b[0m, in \u001b[0;36mLlamaWithLMClassifier.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs7643-llm/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1101\u001b[0m, in \u001b[0;36mLlamaForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m-> 1101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs7643-llm/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:884\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 884\u001b[0m     [\u001b[43mLlamaDecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[1;32m    885\u001b[0m )\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs7643-llm/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:679\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.__init__\u001b[0;34m(self, config, layer_idx)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m LLAMA_ATTENTION_CLASSES[config\u001b[38;5;241m.\u001b[39m_attn_implementation](config\u001b[38;5;241m=\u001b[39mconfig, layer_idx\u001b[38;5;241m=\u001b[39mlayer_idx)\n\u001b[0;32m--> 679\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs7643-llm/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:192\u001b[0m, in \u001b[0;36mLlamaMLP.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mintermediate_size\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_size, bias\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmlp_bias)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size, bias\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmlp_bias)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs7643-llm/lib/python3.12/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs7643-llm/lib/python3.12/site-packages/torch/nn/modules/linear.py:109\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs7643-llm/lib/python3.12/site-packages/torch/nn/init.py:459\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[1;32m    457\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config, tokenizer, model = _load_model(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_args_to_results(args, results):\n",
    "    # Save results in a dataframe\n",
    "    results[\"task_description\"] = args.task_description if args.task_description is not None else \" \"\n",
    "    results[\"pattern\"] = args.pattern\n",
    "    results[\"target_tokens\"] = args.target_tokens\n",
    "    results[\"num_shots\"] = args.num_shots\n",
    "    results[\"separate_shots_by\"] = args.separate_shots_by\n",
    "    results[\"balanced\"] = args.balanced\n",
    "    results[\"shuffle\"] = args.shuffle\n",
    "    results[\"target_prefix\"] = args.target_prefix\n",
    "    results[\"group\"] = args.group\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_df(results):\n",
    "    data = {k: [v] for k, v in results.items()}\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "\n",
    "    # Apply a pattern to the inputs\n",
    "    if context != \"\":\n",
    "        # we add the context here\n",
    "        pattern = f\"{context}{in_context_args.pattern}\"\n",
    "    else:\n",
    "        pattern = in_context_args.pattern\n",
    "\n",
    "    if in_context_args.target_prefix != \"\":\n",
    "        pattern = f\"{pattern} {in_context_args.target_prefix.strip()}\"\n",
    "\n",
    "    pattern_examples = [\n",
    "        pattern.format(\n",
    "            text1=examples[sentence1_key][idx],\n",
    "            text2=examples[sentence2_key][idx] if sentence2_key is not None else None)\n",
    "        for idx in range(len(examples[sentence1_key]))\n",
    "    ]\n",
    "\n",
    "    args = (pattern_examples,)\n",
    "    result = tokenizer(*args, padding=padding,\n",
    "                        max_length=max_seq_length, truncation=True)\n",
    "\n",
    "    # Get tokens\n",
    "    result[\"input_tokens\"] = [tokenizer.convert_ids_to_tokens(\n",
    "        ids) for ids in result[\"input_ids\"]]\n",
    "\n",
    "    # Decode input\n",
    "    result[\"input_text\"] = [tokenizer.decode(\n",
    "        ids) for ids in result[\"input_ids\"]]\n",
    "\n",
    "    # Replace labels by target tokens indices when using lm_head\n",
    "    result[\"label\"] = [target_tokens_ids[l] for l in examples[\"label\"]]\n",
    "    result[\"label_text\"] = [id_to_target_token[l] if l != -1 else \"unlabeled\"\n",
    "                            for l in examples[\"label\"]]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    result = {}\n",
    "\n",
    "    preds = p.predictions[0] if isinstance(\n",
    "        p.predictions, tuple) else p.predictions\n",
    "    labels = p.label_ids\n",
    "    predicted_token_ids = np.argmax(preds, axis=1)\n",
    "    # get the logits for each of the target tokens\n",
    "    class_logits = [[logits[target_tokens_ids[0]], logits[target_tokens_ids[1]]]\n",
    "                    for _, logits in enumerate(preds)]\n",
    "    class_logits = np.asarray(class_logits)\n",
    "\n",
    "    # Compute exact match\n",
    "    result[\"accuracy\"] = np.mean(labels == predicted_token_ids)\n",
    "\n",
    "    # Compute score based performance\n",
    "    # TODO(mm): speed this up\n",
    "    scores = []\n",
    "    for idx, batch_logits in enumerate(class_logits):\n",
    "        # we get the class id of the label token\n",
    "        class_id = token_id_to_label_id[labels[idx]]\n",
    "        # does it receive larger probability than the other classes?\n",
    "        predicted_token_class = np.argmax(batch_logits)\n",
    "        score = predicted_token_class == class_id\n",
    "        scores.append(score)\n",
    "\n",
    "    scores = np.asarray(scores)\n",
    "    result[\"score_accuracy\"] = np.mean(scores)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args: ModelArguments = ModelArguments(\n",
    "    model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    use_auth_token=hf_token,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args: DataTrainingArguments = DataTrainingArguments(\n",
    "    task_name=\"mnli\",\n",
    "    max_seq_length=2048,\n",
    "    eval_task_name=\"hans\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_context_args: InContextLearningArguments = InContextLearningArguments(\n",
    "    pattern=\"{text1} question: {text2} Yes or No?\",\n",
    "    target_prefix=\" answer: \",\n",
    "    target_tokens=\"ĠYes,ĠNo\",\n",
    "    separate_shots_by=\"\\n\\n\",\n",
    "    group=\"gpt-3\", # TODO: Change this\n",
    "    num_shots=2, # 2, 16, 32\n",
    "    balanced=True,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args: TrainingArguments = TrainingArguments(\n",
    "    output_dir=\"./\",\n",
    "    do_eval=True,\n",
    "    per_device_eval_batch_size=10,\n",
    "    fp16=True,\n",
    "    seed=0,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/19/2024 01:51:36 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"False\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "\n",
    "# Set seed before initializing model.\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset and validation set for in-domain data\n",
    "if data_args.task_name in [\"rte\", \"mnli\", \"mnli-original\", \"qqp\", \"cola\"]:\n",
    "    raw_datasets, label_list, num_labels, is_regression = load_glue_datasets(\n",
    "        data_args, model_args)\n",
    "\n",
    "additional_evaluation_datasets = {}\n",
    "if data_args.eval_task_name == \"hans\":\n",
    "    for heuristic in [\"lexical_overlap\"]:\n",
    "        # for heuristic in [\"lexical_overlap\", \"subsequence\", \"constituent\"]:\n",
    "        # Load HANS subsets as additional validation data\n",
    "        for label in [0, 1]:\n",
    "            hans_subset, subset_name = load_hans_dataset(\n",
    "                data_args.dataset_cache_dir, heuristic=heuristic, subcase=None, label=label)\n",
    "            additional_evaluation_datasets[subset_name] = hans_subset\n",
    "\n",
    "elif data_args.eval_task_name == \"mnli-mismatched\":\n",
    "    # Load mnli mismatched validation set\n",
    "    for label in [0, 1]:\n",
    "        mnli_mm_subset, subset_name = load_mnli_mismatched_dataset(\n",
    "            data_args, label=label)\n",
    "        additional_evaluation_datasets[subset_name] = mnli_mm_subset\n",
    "\n",
    "elif data_args.eval_task_name == \"paws-qqp\":\n",
    "    for label in [0, 1]:\n",
    "        paws_qqp_subset, subset_name = load_paws_qqp_dataset(\n",
    "            data_args.eval_task_path, label=label, cache_dir=data_args.dataset_cache_dir)\n",
    "        additional_evaluation_datasets[subset_name] = paws_qqp_subset\n",
    "\n",
    "elif data_args.eval_task_name == \"cola-ood\":\n",
    "    for label in [0, 1]:\n",
    "        cola_ood_subset, subset_name = load_cola_ood_dataset(\n",
    "            data_args.eval_task_path, label=label, cache_dir=data_args.dataset_cache_dir)\n",
    "        additional_evaluation_datasets[subset_name] = cola_ood_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Padding_idx must be within num_embeddings",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m config, tokenizer, model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 49\u001b[0m, in \u001b[0;36m_load_model\u001b[0;34m(model_args)\u001b[0m\n\u001b[1;32m     45\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39m_convert_id_to_token(\n\u001b[1;32m     46\u001b[0m     config\u001b[38;5;241m.\u001b[39mpad_token_id)  \u001b[38;5;66;03m# let's use the eos token\u001b[39;00m\n\u001b[1;32m     47\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 49\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaWithLMClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     50\u001b[0m     model_args\u001b[38;5;241m.\u001b[39mmodel_name_or_path,\n\u001b[1;32m     51\u001b[0m     from_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     52\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     53\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mmodel_args\u001b[38;5;241m.\u001b[39mcache_dir,\n\u001b[1;32m     54\u001b[0m     revision\u001b[38;5;241m=\u001b[39mmodel_args\u001b[38;5;241m.\u001b[39mmodel_revision,\n\u001b[1;32m     55\u001b[0m     use_auth_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m model_args\u001b[38;5;241m.\u001b[39muse_auth_token \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     56\u001b[0m     ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mmodel_args\u001b[38;5;241m.\u001b[39mignore_mismatched_sizes,\n\u001b[1;32m     57\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m     58\u001b[0m     quantization_config\u001b[38;5;241m=\u001b[39mbnb_config,\n\u001b[1;32m     59\u001b[0m     attn_implementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     60\u001b[0m )\n\u001b[1;32m     62\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(task_type\u001b[38;5;241m=\u001b[39mTaskType\u001b[38;5;241m.\u001b[39mSEQ_CLS, \n\u001b[1;32m     63\u001b[0m                         inference_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m     64\u001b[0m                         lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m                         bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     68\u001b[0m                         target_modules\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     69\u001b[0m model \u001b[38;5;241m=\u001b[39m get_peft_model(model, peft_config)\n",
      "File \u001b[0;32m/mnt/c/Users/Justus Soh/Desktop/CS7643/llm/foundation/llama_wrapper.py:22\u001b[0m, in \u001b[0;36mLlamaWithLMClassifier.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs7643-llm/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1101\u001b[0m, in \u001b[0;36mLlamaForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m-> 1101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs7643-llm/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:882\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mpad_token_id\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[0;32m--> 882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m    884\u001b[0m     [LlamaDecoderLayer(config, layer_idx) \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[1;32m    885\u001b[0m )\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs7643-llm/lib/python3.12/site-packages/torch/nn/modules/sparse.py:134\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m padding_idx \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 134\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m padding_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_embeddings, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPadding_idx must be within num_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m padding_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m padding_idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_embeddings, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPadding_idx must be within num_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Padding_idx must be within num_embeddings"
     ]
    }
   ],
   "source": [
    "config, tokenizer, model = _load_model(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_args.task_name is not None:\n",
    "    sentence1_key, sentence2_key = task_to_keys[data_args.task_name]\n",
    "else:\n",
    "    # Again, we try to have some nice defaults but don't hesitate to tweak to your use case.\n",
    "    non_label_column_names = [\n",
    "        name for name in raw_datasets[\"train\"].column_names if name != \"label\"]\n",
    "    if \"sentence1\" in non_label_column_names and \"sentence2\" in non_label_column_names:\n",
    "        sentence1_key, sentence2_key = \"sentence1\", \"sentence2\"\n",
    "    else:\n",
    "        if len(non_label_column_names) >= 2:\n",
    "            sentence1_key, sentence2_key = non_label_column_names[:2]\n",
    "        else:\n",
    "            sentence1_key, sentence2_key = non_label_column_names[0], None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding strategy\n",
    "if data_args.pad_to_max_length:\n",
    "    padding = \"max_length\"\n",
    "else:\n",
    "    # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n",
    "    padding = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entailment': 0, 'contradiction': 1}\n",
      "{0: 'LABEL_0', 1: 'LABEL_1'}\n"
     ]
    }
   ],
   "source": [
    "# Some models have set the order of the labels to use, so let's make sure we do use it.\n",
    "label_to_id = None\n",
    "if (\n",
    "    model.config.label2id != PretrainedConfig(\n",
    "        num_labels=num_labels).label2id\n",
    "    and data_args.task_name is not None\n",
    "    and not is_regression\n",
    "):\n",
    "    # Some have all caps in their config, some don't.\n",
    "    label_name_to_id = {\n",
    "        k.lower(): v for k, v in model.config.label2id.items()}\n",
    "    if list(sorted(label_name_to_id.keys())) == list(sorted(label_list)):\n",
    "        label_to_id = {\n",
    "            i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}\n",
    "    else:\n",
    "        logger.warning(\n",
    "            \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n",
    "            f\"model labels: {list(sorted(label_name_to_id.keys()))}, dataset labels: {list(sorted(label_list))}.\"\n",
    "            \"\\nIgnoring the model labels as a result.\",\n",
    "        )\n",
    "elif data_args.task_name is None and not is_regression:\n",
    "    label_to_id = {v: i for i, v in enumerate(label_list)}\n",
    "\n",
    "if label_to_id is not None:\n",
    "    model.config.label2id = label_to_id\n",
    "    model.config.id2label = {\n",
    "        id: label for label, id in config.label2id.items()}\n",
    "elif data_args.task_name is not None and not is_regression:\n",
    "    model.config.label2id = {l: i for i, l in enumerate(label_list)}\n",
    "    model.config.id2label = {\n",
    "        id: label for label, id in config.label2id.items()}\n",
    "\n",
    "print(model.config.label2id)\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map targets to ids and vice versa\n",
    "target_tokens = [t.strip()\n",
    "                    for t in in_context_args.target_tokens.split(\",\")]\n",
    "target_tokens_ids = tokenizer.convert_tokens_to_ids(target_tokens)\n",
    "id_to_target_token = {idx: t for idx, t in enumerate(target_tokens)}\n",
    "target_token_to_id = {t: idx for idx, t in enumerate(target_tokens)}\n",
    "token_id_to_label_id = {tidx: lidx for lidx,\n",
    "                        tidx in enumerate(target_tokens_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute max_seq_length\n",
    "if data_args.max_seq_length > tokenizer.model_max_length:\n",
    "    logger.warning(\n",
    "        f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the\"\n",
    "        f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "    )\n",
    "\n",
    "max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 261802/261802 [00:00<00:00, 387053.98 examples/s]\n",
      "Filter: 100%|██████████| 261802/261802 [00:00<00:00, 518197.79 examples/s]\n",
      "Filter: 100%|██████████| 261802/261802 [00:00<00:00, 584691.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Create in-context learning prompt from training data\n",
    "context, contex_indices = create_few_shot_context(\n",
    "    data_args.task_name, raw_datasets[\"train\"], in_context_args.num_shots, pattern=in_context_args.pattern,\n",
    "    label_to_tokens=id_to_target_token,\n",
    "    separate_shots_by=in_context_args.separate_shots_by, description=in_context_args.task_description,\n",
    "    target_prefix=in_context_args.target_prefix,\n",
    "    from_indices=in_context_args.sample_indices_file, balanced=in_context_args.balanced, shuffle=in_context_args.shuffle,\n",
    "    seed=training_args.data_seed\n",
    ")\n",
    "# inspect context\n",
    "logger.info(\"Using the following context:\")\n",
    "logger.info(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize context\n",
    "result = tokenizer(context, padding=padding,\n",
    "                    max_length=max_seq_length, truncation=False)\n",
    "# print(result[\"input_ids\"])\n",
    "# print(len(result[\"input_ids\"]))\n",
    "if len(result[\"input_ids\"]) > max_seq_length:\n",
    "    # we skip the current run. The context is too long\n",
    "    print(\"Context is too long. Skipping run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 6692/6692 [00:00<00:00, 19046.56 examples/s]\n",
      "Casting the dataset: 100%|██████████| 5000/5000 [00:00<00:00, 12790.47 examples/s]\n",
      "Casting the dataset: 100%|██████████| 5000/5000 [00:00<00:00, 13095.14 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 6692/6692 [00:36<00:00, 181.80 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 5000/5000 [00:27<00:00, 183.13 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 5000/5000 [00:27<00:00, 179.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "if training_args.do_eval:\n",
    "    # Get the in-domain validation dataset\n",
    "    eval_dataset = raw_datasets[\"validation_matched\" if data_args.task_name in\n",
    "                                [\"mnli\", \"mnli-original\"] else \"validation\"]\n",
    "\n",
    "    # (optional) subsample eval datasets\n",
    "    if data_args.max_eval_samples is not None:\n",
    "        # we fix the random seed that controls the sampling\n",
    "        # we need to uses a fixed seed here to make sure we evaluate on the same data\n",
    "        np.random.seed(123)\n",
    "\n",
    "        max_eval_samples = min(\n",
    "            len(eval_dataset), data_args.max_eval_samples)\n",
    "        # randomly select a subset of the eval data\n",
    "        indices = np.random.choice(\n",
    "            range(len(eval_dataset)), size=max_eval_samples, replace=False)\n",
    "        eval_dataset = eval_dataset.select(indices)\n",
    "\n",
    "    for name, dataset in additional_evaluation_datasets.items():\n",
    "        if data_args.max_eval_samples is not None:\n",
    "            # we fix the random seed that controls the sampling\n",
    "            # we need to uses a fixed seed here to make sure we evaluate on the same data\n",
    "            np.random.seed(123)\n",
    "\n",
    "            max_eval_samples = min(\n",
    "                len(dataset), data_args.max_eval_samples)\n",
    "            # randomly select a subset of the eval data\n",
    "            indices = np.random.choice(\n",
    "                range(len(dataset)), size=max_eval_samples, replace=False)\n",
    "            dataset = dataset.select(indices)\n",
    "            additional_evaluation_datasets[name] = dataset\n",
    "\n",
    "    # set all random seeds again (not sure if this is really needed)\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # We need to update the number of classes of the dataset when using the lm_head\n",
    "    if in_context_args.target_tokens is not None and not in_context_args.target_tokens_logits_only:\n",
    "        new_features = eval_dataset.features.copy()\n",
    "        names = [f\"{idx}\" for idx in np.arange(len(tokenizer))]\n",
    "        new_features[\"label\"] = ClassLabel(\n",
    "            names=names, num_classes=len(tokenizer))\n",
    "        eval_dataset = eval_dataset.cast(new_features)\n",
    "\n",
    "        for name, dataset in additional_evaluation_datasets.items():\n",
    "            new_features = dataset.features.copy()\n",
    "            names = [f\"{idx}\" for idx in np.arange(len(tokenizer))]\n",
    "            new_features[\"label\"] = ClassLabel(\n",
    "                names=names, num_classes=len(tokenizer))\n",
    "            additional_evaluation_datasets[name] = dataset.cast(\n",
    "                new_features)\n",
    "\n",
    "    # Tokenize and encode validation datasets\n",
    "    with training_args.main_process_first(desc=\"dataset map pre-processing\"):\n",
    "        eval_dataset = eval_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            load_from_cache_file=False,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n",
    "\n",
    "        for name, dataset in additional_evaluation_datasets.items():\n",
    "            sentence1_key, sentence2_key = task_to_keys[data_args.eval_task_name]\n",
    "            dataset = dataset.map(\n",
    "                preprocess_function,\n",
    "                batched=True,\n",
    "                batch_size=1000,\n",
    "                load_from_cache_file=False,\n",
    "                desc=\"Running tokenizer on dataset\",\n",
    "            )\n",
    "            additional_evaluation_datasets[name] = dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 6209 of the validation set: {'premise': 'Piccadilly Tube station.', 'hypothesis': 'Euston train station. ', 'label': 2360, 'idx': 9058, 'input_ids': [128000, 11, 872, 19564, 4097, 502, 14324, 705, 5426, 14324, 1053, 5376, 555, 400, 17, 11, 19272, 10826, 400, 19, 11, 931, 5376, 304, 4443, 14324, 2753, 279, 400, 16, 11, 4364, 18979, 304, 3109, 14324, 13, 3488, 25, 5426, 14324, 1053, 18979, 555, 400, 17, 11, 19272, 7566, 477, 2360, 30, 4320, 25, 2360, 271, 13699, 11, 1521, 12074, 4510, 11, 304, 6485, 15082, 279, 1690, 11434, 889, 527, 12207, 6532, 617, 2204, 50446, 304, 872, 16540, 315, 4455, 323, 1524, 304, 872, 51866, 315, 1148, 7077, 11, 323, 420, 374, 1888, 18545, 449, 264, 1749, 430, 27115, 11105, 323, 44928, 82, 1521, 5361, 50446, 11, 4856, 1109, 264, 1749, 430, 22204, 264, 3254, 8206, 6866, 13, 3488, 25, 59250, 1781, 430, 264, 3254, 8206, 1749, 374, 539, 1888, 18545, 449, 264, 1749, 16239, 2204, 50446, 13, 7566, 477, 2360, 30, 4320, 25, 7566, 271, 34905, 35555, 14722, 30124, 8216, 13, 3488, 25, 469, 592, 263, 5542, 8216, 13, 220, 7566, 477, 2360, 30, 4320, 25, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_tokens': ['<|begin_of_text|>', ',', 'Ġtheir', 'Ġcontributions', 'Ġrepresent', 'Ġnew', 'Ġsaving', '),', 'Ġnational', 'Ġsaving', 'Ġwould', 'Ġincrease', 'Ġby', 'Ġ$', '2', ',', '880', '-the', 'Ġ$', '4', ',', '000', 'Ġincrease', 'Ġin', 'Ġpersonal', 'Ġsaving', 'Ġless', 'Ġthe', 'Ġ$', '1', ',', '120', 'Ġdecrease', 'Ġin', 'Ġgovernment', 'Ġsaving', '.', 'Ġquestion', ':', 'Ġnational', 'Ġsaving', 'Ġwould', 'Ġdecrease', 'Ġby', 'Ġ$', '2', ',', '880', 'ĠYes', 'Ġor', 'ĠNo', '?', 'Ġanswer', ':', 'ĠNo', 'ĊĊ', 'Also', ',', 'Ġthese', 'Ġresearchers', 'Ġbelieve', ',', 'Ġin', 'Ġcomplex', 'Ġsituations', 'Ġthe', 'Ġmany', 'Ġpersons', 'Ġwho', 'Ġare', 'Ġsignificantly', 'Ġinvolved', 'Ġhave', 'Ġdifferent', 'Ġrealities', 'Ġin', 'Ġtheir', 'Ġexplanation', 'Ġof', 'Ġevents', 'Ġand', 'Ġeven', 'Ġin', 'Ġtheir', 'Ġperceptions', 'Ġof', 'Ġwhat', 'Ġhappened', ',', 'Ġand', 'Ġthis', 'Ġis', 'Ġbest', 'Ġmatched', 'Ġwith', 'Ġa', 'Ġmethod', 'Ġthat', 'Ġgradually', 'Ġrepresents', 'Ġand', 'Ġreconstruct', 's', 'Ġthese', 'Ġmultiple', 'Ġrealities', ',', 'Ġrather', 'Ġthan', 'Ġa', 'Ġmethod', 'Ġthat', 'Ġassumes', 'Ġa', 'Ġsingle', 'Ġtruth', 'Ġexists', '.', 'Ġquestion', ':', 'ĠResearchers', 'Ġthink', 'Ġthat', 'Ġa', 'Ġsingle', 'Ġtruth', 'Ġmethod', 'Ġis', 'Ġnot', 'Ġbest', 'Ġmatched', 'Ġwith', 'Ġa', 'Ġmethod', 'Ġinvolving', 'Ġdifferent', 'Ġrealities', '.', 'ĠYes', 'Ġor', 'ĠNo', '?', 'Ġanswer', ':', 'ĠYes', 'ĊĊ', 'Pic', 'cad', 'illy', 'ĠTube', 'Ġstation', '.', 'Ġquestion', ':', 'ĠE', 'ust', 'on', 'Ġtrain', 'Ġstation', '.', 'Ġ', 'ĠYes', 'Ġor', 'ĠNo', '?', 'Ġanswer', ':', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>', '<|eot_id|>'], 'input_text': '<|begin_of_text|>, their contributions represent new saving), national saving would increase by $2,880-the $4,000 increase in personal saving less the $1,120 decrease in government saving. question: national saving would decrease by $2,880 Yes or No? answer: No\\n\\nAlso, these researchers believe, in complex situations the many persons who are significantly involved have different realities in their explanation of events and even in their perceptions of what happened, and this is best matched with a method that gradually represents and reconstructs these multiple realities, rather than a method that assumes a single truth exists. question: Researchers think that a single truth method is not best matched with a method involving different realities. Yes or No? answer: Yes\\n\\nPiccadilly Tube station. question: Euston train station.  Yes or No? answer:<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>', 'label_text': 'ĠNo'}.\n"
     ]
    }
   ],
   "source": [
    "# Log a few random samples from the validation set:\n",
    "for index in random.sample(range(len(eval_dataset)), 1):\n",
    "    print(f\"Sample {index} of the validation set: {eval_dataset[index]}.\")\n",
    "    logger.info(f\"Sample {index} of the validation set: {eval_dataset[index]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 6692/6692 [00:11<00:00, 568.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the validation set and make sure the last token is a padding token\n",
    "keep_counter = {}\n",
    "keep_indices = []\n",
    "for sample in eval_dataset:\n",
    "    # assert sample[\"input_ids\"][-1] == tokenizer.pad_token_id, sample[\"input_text\"]\n",
    "    if sample[\"input_ids\"][-1] == tokenizer.pad_token_id:\n",
    "        # the last position is a padding token\n",
    "        keep_indices.append(sample[\"idx\"])\n",
    "# keep only those eval samples that fit into the context\n",
    "keep_num_samples = len(keep_indices)\n",
    "if keep_num_samples > 0:\n",
    "    logger.info(f\"Keeping {keep_num_samples} validation examples\")\n",
    "    print(f\"Keeping {keep_num_samples} validation examples\")\n",
    "    eval_dataset = _select_subset_by_idx(eval_dataset, keep_indices)\n",
    "    keep_counter[\"in-domain\"] = keep_num_samples\n",
    "else:\n",
    "    logger.info(\"Skipping the current run. The prompt is too long.\")\n",
    "    print(\"Skipping the current run. The prompt is too long.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping 5000 validation examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5000/5000 [00:08<00:00, 604.92 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping 5000 validation examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5000/5000 [00:08<00:00, 585.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# for ood\n",
    "additional_evaluation_datasets_tmp = {}\n",
    "for name, dataset in additional_evaluation_datasets.items():\n",
    "    keep_indices = []\n",
    "    for sample in dataset:\n",
    "        # assert sample[\"input_ids\"][-1] == tokenizer.pad_token_id, sample[\"input_text\"]\n",
    "        if sample[\"input_ids\"][-1] == tokenizer.pad_token_id:\n",
    "            keep_indices.append(sample[\"idx\"])\n",
    "    # keep only those eval samples that fit into the context\n",
    "    keep_num_samples = len(keep_indices)\n",
    "    if keep_num_samples > 0:\n",
    "        logger.info(f\"Keeping {keep_num_samples} validation examples\")\n",
    "        print(f\"Keeping {keep_num_samples} validation examples\")\n",
    "        tmp_dataset = _select_subset_by_idx(dataset, keep_indices)\n",
    "        additional_evaluation_datasets_tmp[name] = tmp_dataset\n",
    "        keep_counter[name] = keep_num_samples\n",
    "    else:\n",
    "        logger.info(\"Skipping the current run. The prompt is too long.\")\n",
    "        print(\"Skipping the current run. The prompt is too long.\")\n",
    "\n",
    "additional_evaluation_datasets = additional_evaluation_datasets_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator will default to DataCollatorWithPadding when the tokenizer is passed to Trainer, so we change it if\n",
    "# we already did the padding.\n",
    "if data_args.pad_to_max_length:\n",
    "    data_collator = default_data_collator\n",
    "elif training_args.fp16:\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer, pad_to_multiple_of=8)\n",
    "else:\n",
    "    data_collator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = FtTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=None,\n",
    "    eval_dataset=None,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    data_args=data_args,\n",
    "    eval_only=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|logging.py:328] 2024-07-19 03:46:12,424 >> We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "if training_args.do_eval:\n",
    "    logger.info(\"*** In-context learning evaluation ***\")\n",
    "\n",
    "    # Get datasets\n",
    "    eval_task_names = [data_args.task_name]\n",
    "    eval_task_names += [task_name for task_name in additional_evaluation_datasets.keys()]\n",
    "    eval_datasets = [eval_dataset]\n",
    "    eval_datasets += [dataset for _,\n",
    "                        dataset in additional_evaluation_datasets.items()]\n",
    "\n",
    "    all_results = {}\n",
    "    for task_name, dataset in zip(eval_task_names, eval_datasets):\n",
    "        outputs = trainer.predict(\n",
    "            dataset, metric_key_prefix=task_name, ignore_keys=[\"past_key_values\"])\n",
    "        predictions = outputs.predictions\n",
    "        labels = outputs.label_ids\n",
    "        metrics = outputs.metrics\n",
    "        all_results = {**metrics, **all_results}\n",
    "        # output_predict_file = os.path.join(\n",
    "        #     training_args.output_dir, f\"predict_results_{task}.txt\")\n",
    "\n",
    "    if trainer.is_world_process_zero():\n",
    "\n",
    "        #     with open(output_predict_file, \"w\") as writer:\n",
    "        #         logger.info(f\"***** Predict results {task} *****\")\n",
    "        #         writer.write(\"index\\tprediction\\n\")\n",
    "        #         for index, item in enumerate(predictions):\n",
    "        #             if is_regression:\n",
    "        #                 writer.write(f\"{index}\\t{item:3.3f}\\n\")\n",
    "        #             else:\n",
    "        #                 item = label_list[item]\n",
    "        #                 writer.write(f\"{index}\\t{item}\\n\")\n",
    "\n",
    "        # Save everything to in a dataframe\n",
    "        all_results = _add_args_to_results(in_context_args, all_results)\n",
    "        all_results[\"indices\"] = contex_indices\n",
    "        all_results[\"context\"] = context\n",
    "        all_results[\"data_seed\"] = training_args.data_seed\n",
    "        all_results[\"keep_samples_in-domain\"] = keep_counter[\"in-domain\"]\n",
    "        for name in additional_evaluation_datasets.keys():\n",
    "            all_results[f\"keep_samples_{name}\"] = keep_counter[name]\n",
    "\n",
    "        df = _create_df(all_results)\n",
    "\n",
    "        if \"llama\" in model_args.model_name_or_path:\n",
    "            name = model_args.model_name_or_path.split(\"/\")\n",
    "            MODEL_NAME = f\"{name[-3]}-{name[-2]}-{name[-1]}\"\n",
    "\n",
    "            file_name = f\"{MODEL_NAME}\" + \\\n",
    "                f\"_{data_args.task_name}\" + \\\n",
    "                f\"_{data_args.eval_task_name}\"\n",
    "\n",
    "        else:\n",
    "            file_name = f\"{model_args.model_name_or_path.replace('/', '-')}\" + \\\n",
    "                f\"_{data_args.task_name}\" + \\\n",
    "                f\"_{data_args.eval_task_name}\"\n",
    "\n",
    "        output_file = os.path.join(\n",
    "            training_args.output_dir, f\"{file_name}.csv\")\n",
    "        if os.path.exists(output_file):\n",
    "            # if the file already exists, we append to it\n",
    "            df.to_csv(output_file, mode='a', header=False)\n",
    "        else:\n",
    "            df.to_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7643-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
